{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import webbrowser\n",
    "import urllib\n",
    "import re\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "#browser = Browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search=input(\"What would you like to search\")\n",
    "#search=search.replace(\" \",\"+\")\n",
    "#link=\"https://www.google.com/search?q=\"+search\n",
    "#link = \"https://www.google.com/maps/search/\"+search\n",
    "#print(link)\n",
    "#source=requests.get(link).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://www.google.com/search?rlz=1C1CHBF_enUS853US853&sz=0&tbm=lcl&ei=05bdXaEyoqX9BoGbkuAK&q=kids+sports+classes+new+jersey&oq=kids+sports+classes+new+jersey&gs_l=psy-ab.12...13265.16654.0.22362.7.7.0.0.0.0.83.504.7.7.0....0...1c.1.64.psy-ab..0.0.0....0.EEUl6U13Yy0#rlfi=hd:;si:;mv:[[41.1122716,-73.93472659999999],[40.3136717,-74.6914486]];tbs:lrf:!1m4!1u3!2m2!3m1!1e1!2m1!1e3!3sIAE,lf:1,lf_ui:3'\n",
    "#url = soup.find_(\"svg\",focusable = {\"false\": \"http://www.w3.org/2000/svg\"})\n",
    "browser.visit(url)\n",
    "\n",
    "browser.find_by_name('btnG').first.click()\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "print(len(Data))\n",
    "top_result = []\n",
    "    \n",
    "for tr in Data:\n",
    "    dataframe = {}\n",
    "    dataframe[\"Name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "    dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "    top_result.append(dataframe)\n",
    "print(top_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-51b6002093b6>, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-51b6002093b6>\"\u001b[1;36m, line \u001b[1;32m100\u001b[0m\n\u001b[1;33m    print(orgAddress)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "Data = []\n",
    "geolocator = Nominatim()\n",
    "geolocator = Nominatim()\n",
    "pageData = soup.find_all(class_='std')\n",
    "#print( soup.find(\"div\", class_=\"std\")[\"aria-label\"] )\n",
    "\n",
    "print(len(pageData))\n",
    "for tr in pageData:\n",
    "    \n",
    "    pLink = str(tr.find('a')['href'])\n",
    "    print(pLink)\n",
    "\n",
    "top_result = []\n",
    "list1 = [2, 4, 6,8,10,12] \n",
    "for x in (list1):\n",
    "    prePage = x\n",
    "    print(prePage)\n",
    "    startPage = \"start=\"+ str(prePage) +\"0\"\n",
    "    print(startPage)\n",
    "    replacePage = \"start=\"+ str(prePage+2) +\"0\"\n",
    "    print(replacePage)\n",
    "    pLink = pLink.replace(startPage,replacePage)\n",
    "    pageLink = \"https://www.google.com\" + pLink\n",
    "    print(pageLink)\n",
    "    browser.visit(pageLink)\n",
    "\n",
    "  \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "    print(len(Data))\n",
    "    \n",
    "    for tr in Data:\n",
    "        \n",
    "        dataframe = {}\n",
    "        dataframe[\"Name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "        dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "    \n",
    "        PhoneData = tr.find('span').text.split(' ')\n",
    "        print(PhoneData)\n",
    "        lPhoneData = len(PhoneData)\n",
    "        phoneNo = PhoneData[lPhoneData-2]+ PhoneData[lPhoneData-1]\n",
    "        phoneNo = re.sub(\"\\D\", \"\", phoneNo)\n",
    "        dataframe[\"phoneNo\"] = phoneNo\n",
    "        \n",
    "        dirClass = (tr.find('div', {'class':'UbRuwe'}))\n",
    "        #print(dirClass)\n",
    "        if dirClass != None :\n",
    "        #if hasattr('a', 'cursor:pointer'):\n",
    "            rawAddress = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf VByer\"})[\"data-url\"])\n",
    "            \n",
    "            orgAddress = rawAddress.split('/')[4].strip()\n",
    "            print(orgAddress)\n",
    "            orgAddress = orgAddress.replace('+',' ')\n",
    "            orgAddress = orgAddress.split(',')\n",
    "            \n",
    "            print(orgAddress)\n",
    "            lenAddress = len(orgAddress)\n",
    "            \n",
    "            if lenAddress == 4:\n",
    "                ZipCode = orgAddress[3].split(' ')\n",
    "                #print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"Address\"] = orgAddress[1]\n",
    "                dataframe[\"City\"] = orgAddress[2]\n",
    "                dataframe[\"State\"] = ZipCode[1]\n",
    "                if len(ZipCode) > 2:\n",
    "                    dataframe[\"Zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "                    \n",
    "            if lenAddress > 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"Address\"] = orgAddress[lenAddress-3]\n",
    "                dataframe[\"City\"] = orgAddress[lenAddress-2]\n",
    "                dataframe[\"State\"] = ZipCode[1]\n",
    "                if len(ZipCode) > 2:\n",
    "                    dataframe[\"Zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                \n",
    "            if lenAddress < 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(orgAddress)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"Address\"] = orgAddress[lenAddress-2]\n",
    "                dataframe[\"City\"] = orgAddress[lenAddress-1]\n",
    "                dataframe[\"State\"] = ZipCode[1]\n",
    "                if len(ZipCode) > 2:\n",
    "                    dataframe[\"Zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                print(orgAddress)\n",
    "                    print(orgAddress)\n",
    "            try:\n",
    "                locForLatLong = dataframe[\"Address\"] + dataframe[\"City\"]\n",
    "                print(locForLatLong)\n",
    "                location = geolocator.geocode(locForLatLong, timeout=10)\n",
    "                print(location.address)\n",
    "                print((location.latitude,location.longitude))\n",
    "                dataframe[\"Location\"] = [location.latitude,location.longitude]\n",
    "                \n",
    "            except AttributeError:\n",
    "                print('Cant Find the address')\n",
    "                \n",
    "        top_result.append(dataframe)\n",
    "\n",
    "#print(top_result)\n",
    "df=pd.DataFrame(top_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"outputSports.xlsx\") \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
