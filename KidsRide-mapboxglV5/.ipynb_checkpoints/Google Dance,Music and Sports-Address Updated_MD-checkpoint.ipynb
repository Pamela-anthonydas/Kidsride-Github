{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Dance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries required for scraping\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import webbrowser\n",
    "import urllib\n",
    "import re\n",
    "import pymongo\n",
    "from geopy.geocoders import Nominatim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the crome driver for scraping\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "#browser = Browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Url for main page\n",
    "url = 'https://www.google.com/search?rlz=1C1CHBF_enUS853US853&sz=0&tbm=lcl&ei=Y5HcXZW5CdG05gKT7KzIAw&q=kids+dance+classes+new+jersey&oq=kids+dance+classes+new+jersey&gs_l=psy-ab.3...140911.143275.0.143677.6.6.0.0.0.0.271.904.0j4j1.5.0....0...1c.1.64.psy-ab..1.0.0....0.2IUp-hMGy8U#rlfi=hd:;si:;mv:[[41.0322952,-73.93508229999999],[40.534548400000006,-74.637359]];tbs:lrf:!1m4!1u3!2m2!3m1!1e1!1m4!1u2!2m2!2m1!1e1!2m1!1e2!2m1!1e3!3sIAE,lf:1,lf_ui:2'\n",
    "browser.visit(url)\n",
    "\n",
    "# updating the page by clicking on search button\n",
    "browser.find_by_name('btnG').first.click()\n",
    "\n",
    "# parser\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "# storing all the data from the main class\n",
    "Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "print(len(Data))\n",
    "top_result = []\n",
    "    \n",
    "# search variables Name and reviews within main class using for loop\n",
    "for tr in Data:\n",
    "    dataframe = {}\n",
    "    dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "    #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "    top_result.append(dataframe)\n",
    "print(top_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "start=20\n",
      "start=40\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pLink' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0b48890d326b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mreplacePage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"start=\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprePage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplacePage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mpLink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpLink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartPage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplacePage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mpageLink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.google.com\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpLink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pLink' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Data = []\n",
    "# Using the geo locator for getting Lat and Long\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "\n",
    "# getting the link for next pages \n",
    "\n",
    "pageData = soup.find_all(class_='std')\n",
    "\n",
    "print(len(pageData))\n",
    "for tr in pageData:\n",
    "    \n",
    "    pLink = str(tr.find('a')['href'])\n",
    "    print(pLink)\n",
    "\n",
    "#updating each page link with specific nunmbers to go to next page\n",
    "\n",
    "top_result = []\n",
    "list1 = [2, 4, 6,8,10,12,14,16,18] \n",
    "for x in (list1):\n",
    "    prePage = x\n",
    "    print(prePage)\n",
    "    startPage = \"start=\"+ str(prePage) +\"0\"\n",
    "    print(startPage)\n",
    "    replacePage = \"start=\"+ str(prePage+2) +\"0\"\n",
    "    print(replacePage)\n",
    "    pLink = pLink.replace(startPage,replacePage)\n",
    "    pageLink = \"https://www.google.com\" + pLink\n",
    "    print(pageLink)\n",
    "    browser.visit(pageLink)\n",
    "\n",
    "# parsing pages as the pages updated with above url\n",
    "\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "    print(len(Data))\n",
    "\n",
    "# search variables Name, url, adress, phone no, zipcode within main class using for loop\n",
    "\n",
    "    for tr in Data:\n",
    "        \n",
    "        dataframe = {}\n",
    "        dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "        #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "        dataframe[\"type\"] = \"Dance Classes\"\n",
    "        \n",
    "#if the url is empty handling the exception\n",
    "\n",
    "        try:\n",
    "            dataframe[\"url\"] = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf L48Cpd\"})['href'])\n",
    "            \n",
    "            print(dataframe[\"url\"])\n",
    "        except:\n",
    "            print('Cant Find the url')\n",
    "\n",
    "# scraping for phone number\n",
    "\n",
    "        PhoneData = tr.find('span').text.split(' ')\n",
    "        #print(PhoneData)\n",
    "        lPhoneData = len(PhoneData)\n",
    "        phoneNo = PhoneData[lPhoneData-2]+ PhoneData[lPhoneData-1]\n",
    "        phoneNo = re.sub(\"\\D\", \"\", phoneNo)\n",
    "        dataframe[\"phoneNo\"] = phoneNo\n",
    "        \n",
    "        dirClass = (tr.find('div', {'class':'UbRuwe'}))\n",
    "        #print(dirClass)\n",
    "        \n",
    "# if the main directory is empty, handling the exception\n",
    "        if dirClass != None :\n",
    "        #if hasattr('a', 'cursor:pointer'):\n",
    "        \n",
    "# getting the address\n",
    "            rawAddress = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf VByer\"})[\"data-url\"])\n",
    "\n",
    "# cleaning the address based on number of delimters\n",
    "\n",
    "            orgAddress = rawAddress.split('/')[4].strip()\n",
    "            print(orgAddress)\n",
    "            orgAddress = orgAddress.replace('+',' ')\n",
    "            orgAddress = orgAddress.split(',')\n",
    "            \n",
    "            print(orgAddress)\n",
    "            lenAddress = len(orgAddress)\n",
    "            \n",
    "            if lenAddress == 4:\n",
    "                ZipCode = orgAddress[3].split(' ')\n",
    "                #print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[1]+', ' + orgAddress[2]+', ' + ZipCode[1]+', ' + ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "                    \n",
    "            if lenAddress > 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-3]+', '+orgAddress[lenAddress-2]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                \n",
    "            if lenAddress < 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(orgAddress)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-2]+', '+orgAddress[lenAddress-1]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-1]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                #    print(orgAddress)\n",
    "                \n",
    "# Using geolocator to find the lat and long for each address\n",
    "\n",
    "            try:\n",
    "                locForLatLong = dataframe[\"address\"] #+ dataframe[\"city\"]\n",
    "                print(locForLatLong)\n",
    "                location = geolocator.geocode(locForLatLong, timeout=10)\n",
    "                print(location.address)\n",
    "                print((location.latitude,location.longitude))\n",
    "                dataframe[\"location\"] = [location.latitude,location.longitude]\n",
    "                \n",
    "            except AttributeError:\n",
    "                print('Cant Find the address')\n",
    "                \n",
    "                \n",
    "        top_result.append(dataframe)\n",
    "\n",
    "#print(top_result)\n",
    "\n",
    "# creating a dataframe for Dance\n",
    "df1=pd.DataFrame(top_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_excel(\"outputDance.xlsx\")\n",
    "\n",
    "# creating a dataframe for Dance\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search?rlz=1C1CHBF_enUS853US853&tbm=lcl&sxsrf=ACYBGNTdFaDd4jAeC6lZKeILHtxz3pMx4A%3A1574389394556&ei=kkbXXanHIeSV_Qbz-a_wAg&q=kids+music+classes+new+jersey&oq=kids+music+classes+new+jersey&gs_l=psy-ab.12...0.0.0.163915.0.0.0.0.0.0.0.0..0.0....0...1c..64.psy-ab..0.0.0....0.51R-_vCUfl4#rlfi=hd:;si:;mv:[[41.0328425,-73.9399237],[40.1005597,-74.3127616]];tbs:lrf:!1m4!1u3!2m2!3m1!1e1!1m4!1u2!2m2!2m1!1e1!1m4!1u16!2m2!16m1!1e1!1m4!1u16!2m2!16m1!1e2!2m1!1e2!2m1!1e3!2m1!1e16!3sIAE,lf:1,lf_ui:2'\n",
    "browser.visit(url)\n",
    "\n",
    "browser.find_by_name('btnG').first.click()\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "print(len(Data))\n",
    "top_result = []\n",
    "    \n",
    "for tr in Data:\n",
    "    dataframe = {}\n",
    "    dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "    #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "    top_result.append(dataframe)\n",
    "print(top_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = []\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "pageData = soup.find_all(class_='std')\n",
    "\n",
    "print(len(pageData))\n",
    "for tr in pageData:\n",
    "    \n",
    "    pLink = str(tr.find('a')['href'])\n",
    "    print(pLink)\n",
    "\n",
    "top_result = []\n",
    "list1 = [2, 4, 6,8,10,12,14,16,18] \n",
    "for x in (list1):\n",
    "    prePage = x\n",
    "    print(prePage)\n",
    "    startPage = \"start=\"+ str(prePage) +\"0\"\n",
    "    print(startPage)\n",
    "    replacePage = \"start=\"+ str(prePage+2) +\"0\"\n",
    "    print(replacePage)\n",
    "    pLink = pLink.replace(startPage,replacePage)\n",
    "    pageLink = \"https://www.google.com\" + pLink\n",
    "    print(pageLink)\n",
    "    browser.visit(pageLink)\n",
    "\n",
    "  \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "    print(len(Data))\n",
    "    \n",
    "    for tr in Data:\n",
    "        \n",
    "        dataframe = {}\n",
    "        dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "        #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "        dataframe[\"type\"] = \"Music Classes\"\n",
    "        try:\n",
    "            dataframe[\"url\"] = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf L48Cpd\"})['href'])\n",
    "            \n",
    "            print(dataframe[\"url\"])\n",
    "        except:\n",
    "            print('Cant Find the url')\n",
    "            \n",
    "        \n",
    "        PhoneData = tr.find('span').text.split(' ')\n",
    "        #print(PhoneData)\n",
    "        lPhoneData = len(PhoneData)\n",
    "        phoneNo = PhoneData[lPhoneData-2]+ PhoneData[lPhoneData-1]\n",
    "        phoneNo = re.sub(\"\\D\", \"\", phoneNo)\n",
    "        dataframe[\"phoneNo\"] = phoneNo\n",
    "        \n",
    "        dirClass = (tr.find('div', {'class':'UbRuwe'}))\n",
    "        #print(dirClass)\n",
    "        if dirClass != None :\n",
    "        #if hasattr('a', 'cursor:pointer'):\n",
    "            rawAddress = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf VByer\"})[\"data-url\"])\n",
    "            \n",
    "            orgAddress = rawAddress.split('/')[4].strip()\n",
    "            print(orgAddress)\n",
    "            orgAddress = orgAddress.replace('+',' ')\n",
    "            orgAddress = orgAddress.split(',')\n",
    "            \n",
    "            print(orgAddress)\n",
    "            lenAddress = len(orgAddress)\n",
    "            \n",
    "            if lenAddress == 4:\n",
    "                ZipCode = orgAddress[3].split(' ')\n",
    "                #print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[1]+', ' + orgAddress[2]+', ' + ZipCode[1]+', ' + ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "                    \n",
    "            if lenAddress > 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-3]+', '+orgAddress[lenAddress-2]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                \n",
    "            if lenAddress < 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(orgAddress)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-2]+', '+orgAddress[lenAddress-1]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-1]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                \n",
    "                    #print(orgAddress)\n",
    "            try:\n",
    "                locForLatLong = dataframe[\"address\"] #+ dataframe[\"city\"]\n",
    "                print(locForLatLong)\n",
    "                location = geolocator.geocode(locForLatLong, timeout=10)\n",
    "                print(location.address)\n",
    "                print((location.latitude,location.longitude))\n",
    "                dataframe[\"location\"] = [location.latitude,location.longitude]\n",
    "                \n",
    "            except AttributeError:\n",
    "                print('Cant Find the address')\n",
    "                \n",
    "                \n",
    "        top_result.append(dataframe)\n",
    "\n",
    "#print(top_result)\n",
    "df2=pd.DataFrame(top_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"outputMusic.xlsx\")\n",
    "\n",
    "# Creating a dataframe for Music\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search?rlz=1C1CHBF_enUS853US853&sz=0&tbm=lcl&ei=05bdXaEyoqX9BoGbkuAK&q=kids+sports+classes+new+jersey&oq=kids+sports+classes+new+jersey&gs_l=psy-ab.12...13265.16654.0.22362.7.7.0.0.0.0.83.504.7.7.0....0...1c.1.64.psy-ab..0.0.0....0.EEUl6U13Yy0#rlfi=hd:;si:;mv:[[41.1122716,-73.93472659999999],[40.3136717,-74.6914486]];tbs:lrf:!1m4!1u3!2m2!3m1!1e1!2m1!1e3!3sIAE,lf:1,lf_ui:3'\n",
    "browser.visit(url)\n",
    "\n",
    "browser.find_by_name('btnG').first.click()\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "time.sleep(5)\n",
    "\n",
    "Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "print(len(Data))\n",
    "top_result = []\n",
    "    \n",
    "for tr in Data:\n",
    "    dataframe = {}\n",
    "    dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "    #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "    top_result.append(dataframe)\n",
    "print(top_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = []\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "pageData = soup.find_all(class_='std')\n",
    "#print( soup.find(\"div\", class_=\"std\")[\"aria-label\"] )\n",
    "\n",
    "print(len(pageData))\n",
    "for tr in pageData:\n",
    "    \n",
    "    pLink = str(tr.find('a')['href'])\n",
    "    print(pLink)\n",
    "\n",
    "top_result = []\n",
    "list1 = [2, 4, 6,8,10,12] \n",
    "for x in (list1):\n",
    "    prePage = x\n",
    "    print(prePage)\n",
    "    startPage = \"start=\"+ str(prePage) +\"0\"\n",
    "    print(startPage)\n",
    "    replacePage = \"start=\"+ str(prePage+2) +\"0\"\n",
    "    print(replacePage)\n",
    "    pLink = pLink.replace(startPage,replacePage)\n",
    "    pageLink = \"https://www.google.com\" + pLink\n",
    "    print(pageLink)\n",
    "    browser.visit(pageLink)\n",
    "\n",
    "  \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    Data = soup.find_all(\"div\",attrs = {\"class\": \"VkpGBb\"})\n",
    "\n",
    "    print(len(Data))\n",
    "    \n",
    "    for tr in Data:\n",
    "        \n",
    "        dataframe = {}\n",
    "        dataframe[\"name\"] = (tr.find(\"div\",attrs = {\"class\": \"dbg0pd\"})).text.replace('\\n', ' ')\n",
    "        #dataframe[\"Reviews\"] = (tr.find('span', {'class':'BTtC6e'}))\n",
    "        dataframe[\"type\"] = \"Sports Classes\"\n",
    "        try:\n",
    "            dataframe[\"url\"] = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf L48Cpd\"})['href'])\n",
    "            \n",
    "            print(dataframe[\"url\"])\n",
    "        except:\n",
    "            print('Cant Find the url')\n",
    "            \n",
    "        \n",
    "        PhoneData = tr.find('span').text.split(' ')\n",
    "        print(PhoneData)\n",
    "        lPhoneData = len(PhoneData)\n",
    "        phoneNo = PhoneData[lPhoneData-2]+ PhoneData[lPhoneData-1]\n",
    "        phoneNo = re.sub(\"\\D\", \"\", phoneNo)\n",
    "        dataframe[\"phoneNo\"] = phoneNo\n",
    "        \n",
    "        dirClass = (tr.find('div', {'class':'UbRuwe'}))\n",
    "        #print(dirClass)\n",
    "        if dirClass != None :\n",
    "        #if hasattr('a', 'cursor:pointer'):\n",
    "            rawAddress = (tr.find(\"a\",attrs = {\"class\": \"yYlJEf VByer\"})[\"data-url\"])\n",
    "            \n",
    "            orgAddress = rawAddress.split('/')[4].strip()\n",
    "            print(orgAddress)\n",
    "            orgAddress = orgAddress.replace('+',' ')\n",
    "            orgAddress = orgAddress.split(',')\n",
    "            \n",
    "            print(orgAddress)\n",
    "            lenAddress = len(orgAddress)\n",
    "            \n",
    "            if lenAddress == 4:\n",
    "                ZipCode = orgAddress[3].split(' ')\n",
    "                #print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[1]+', ' + orgAddress[2]+', ' + ZipCode[1]+', ' + ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "                    \n",
    "            if lenAddress > 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(ZipCode)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-3]+', '+orgAddress[lenAddress-2]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-2]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                \n",
    "            if lenAddress < 4 :\n",
    "                ZipCode = orgAddress[lenAddress-1].split(' ')\n",
    "                print(orgAddress)\n",
    "                #print (string_in_slashes.split(','))\n",
    "            \n",
    "                dataframe[\"address\"] = orgAddress[lenAddress-2]+', '+orgAddress[lenAddress-1]+', '+ ZipCode[1]+', '+ZipCode[2]\n",
    "                #dataframe[\"city\"] = orgAddress[lenAddress-1]\n",
    "                #dataframe[\"state\"] = ZipCode[1]\n",
    "                #if len(ZipCode) > 2:\n",
    "                    #dataframe[\"zipcode\"] = ZipCode[2]\n",
    "                    #dataframe[\"Zipcode\"] = orgAddress[4]\n",
    "            \n",
    "                    #print(orgAddress)\n",
    "                    \n",
    "            try:\n",
    "                locForLatLong = dataframe[\"address\"]# + dataframe[\"city\"]\n",
    "                print(locForLatLong)\n",
    "                location = geolocator.geocode(locForLatLong, timeout=10)\n",
    "                print(location.address)\n",
    "                print((location.latitude,location.longitude))\n",
    "                dataframe[\"location\"] = [location.latitude,location.longitude]\n",
    "                \n",
    "            except AttributeError:\n",
    "                print('Cant Find the address')\n",
    "                \n",
    "        top_result.append(dataframe)\n",
    "\n",
    "#print(top_result)\n",
    "df3=pd.DataFrame(top_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"outputSports.xlsx\")\n",
    "\n",
    "# creating a dataframe for sports\n",
    "df3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged all three dataframe into one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all 3 dataframe for music, Dannce and Sports\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "\n",
    "# Cleaning the dataframet to drop null values for location\n",
    "\n",
    "df = df.dropna(axis=0, subset=['location'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting to excel\n",
    "\n",
    "df.to_excel(\"outputKidsActivities.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reseting the index\n",
    "\n",
    "df=df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the column to _id\n",
    "df = df.drop(df.index[0])\n",
    "df = df.rename(columns={'index':'_id'})\n",
    "#df = df.drop(columns=['Reviews'])\n",
    "#df = df.drop(columns=['_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding to directory\n",
    "split_data = df.to_dict('records')\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all types are objects\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the MongoDB connection through pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Create DB\n",
    "\n",
    "db = myclient[\"KidsRidedb3\"]\n",
    "\n",
    "# Create collection and insert all the data into the MongoDB\n",
    "\n",
    "mycol = db[\"activityData6\"]\n",
    "mycol.insert_many(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
